{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nshGZ8GVNe9v"
      },
      "source": [
        "# Enhanced Chunking and Vectorization with Docling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjnLwuCKN08G"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT5y_QW8ebsn"
      },
      "source": [
        "## Installation and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEoM938B_JRH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 10) and sys.version_info < (3, 13), \"Use Python 3.10, 3.11, or 3.12 to run this notebook.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BfMWUUSs_JRI",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2a03c19a-a72a-44dd-c2ba-64e6c25a8681",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "! pip install \"git+https://github.com/ibm-granite-community/utils.git\" \\\n",
        "    transformers \\\n",
        "    pillow \\\n",
        "    langchain_community \\\n",
        "    langchain_huggingface \\\n",
        "    langchain_milvus \\\n",
        "    docling \\\n",
        "    matplotlib \\\n",
        "    replicate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKnuBqIBekTi"
      },
      "source": [
        "Now let's import the essential modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZxWTGAFNIxK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import tempfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from typing import List, Dict, Any, Iterator, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inXV8p7EIvag"
      },
      "source": [
        "### Logging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0jUQOfNIvag"
      },
      "source": [
        "To see detailed information about the document processing and chunking operations, we'll configure INFO log level.\n",
        "\n",
        "NOTE: It is okay to skip running this cell if you prefer less verbose output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQV4OW29Ivag"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49OmkCJKEEc"
      },
      "source": [
        "## Document Processing with Docling\n",
        "\n",
        "### Understanding Document Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xow-PjeOKJA2",
        "outputId": "f2e8dfe7-3f17-4163-bfcc-3d69b42fcac0"
      },
      "outputs": [],
      "source": [
        "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
        "from docling.datamodel.base_models import InputFormat\n",
        "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
        "\n",
        "# Configure pipeline to extract both text and images\n",
        "pdf_pipeline_options = PdfPipelineOptions(\n",
        "    do_ocr=False,   # Skip OCR for faster processing (enable for scanned docs)\n",
        "    generate_picture_images=True,   # Extract images for multi-modal applications\n",
        ")\n",
        "format_options = {\n",
        "    InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_pipeline_options),\n",
        "}\n",
        "converter = DocumentConverter(format_options=format_options)\n",
        "\n",
        "# Process a sample document\n",
        "sample_doc_url = \"https://midwestfoodbank.org/images/AR_2020_WEB2.pdf\"\n",
        "result = converter.convert(sample_doc_url)\n",
        "doc = result.document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP_mj4dBiWFW"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmNeNMIInSH"
      },
      "source": [
        "## Chunk Visualization: See What You're Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY1x4I-nibza"
      },
      "source": [
        "### Building a Comprehensive Chunk Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewCZQfo0IteY"
      },
      "outputs": [],
      "source": [
        "from docling_core.transforms.chunker.tokenizer.base import BaseTokenizer\n",
        "from docling.chunking import BaseChunker, BaseChunk\n",
        "from docling.datamodel.document import DoclingDocument\n",
        "\n",
        "def visualize_chunks(chunks: list[BaseChunk], *, chunker: BaseChunker, tokenizer: BaseTokenizer, title=\"Document Chunks\"):\n",
        "    \"\"\"Visualize chunk sizes and distribution in tokens.\n",
        "    \"\"\"\n",
        "    # Extract token counts for each chunk\n",
        "    token_counts = [tokenizer.count_tokens(chunker.contextualize(chunk=chunk)) for chunk in chunks]\n",
        "\n",
        "    # Create histogram with all annotations in one go\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Create the histogram\n",
        "    plt.hist(token_counts, bins=20, alpha=0.7, color='skyblue')\n",
        "\n",
        "    # Add statistics line and annotations\n",
        "    avg_tokens = np.mean(token_counts)\n",
        "    plt.axvline(avg_tokens, color='red', linestyle='--', label=f'Average: {avg_tokens:.1f}')\n",
        "\n",
        "    # Add labels and formatting\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Chunk Size (tokens)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the complete plot\n",
        "    plt.show()\n",
        "\n",
        "    # Print comprehensive statistics\n",
        "    print(f\"Chunk Analysis Results:\")\n",
        "    print(f\"Total chunks: {len(token_counts)}\")\n",
        "    print(f\"Average chunk size: {np.mean(token_counts):.1f} tokens\")\n",
        "    print(f\"Minimum chunk size: {min(token_counts)} tokens\")\n",
        "    print(f\"Maximum chunk size: {max(token_counts)} tokens\")\n",
        "    print(f\"Standard deviation: {np.std(token_counts):.1f} tokens\")\n",
        "\n",
        "    # Quality indicators\n",
        "    if max(token_counts) > 512:\n",
        "        print(\"Warning: Some chunks exceed 512 tokens - consider reducing chunk size\")\n",
        "    if np.std(token_counts) > 100:\n",
        "        print(\"Warning: High variance in chunk sizes - retrieval consistency may suffer\")\n",
        "\n",
        "    # Also show character length for reference\n",
        "    char_lengths = [len(chunk.text) for chunk in chunks]\n",
        "    print(f\"\\nReference - Average character length: {np.mean(char_lengths):.1f} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEh8rIP-I9mO"
      },
      "source": [
        "## Docling Chunking Fundamentals: Understanding the Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDNwHE0vJKi7"
      },
      "source": [
        "### The BaseChunker Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymJMe-hLJKEQ"
      },
      "outputs": [],
      "source": [
        "# Example of BaseChunker interface structure\n",
        "class SimpleChunker(BaseChunker):\n",
        "    \"\"\"A simple example chunker implementing the BaseChunker interface.\"\"\"\n",
        "\n",
        "    def chunk(self, dl_doc: DoclingDocument, **kwargs) -> Iterator[BaseChunk]:\n",
        "        \"\"\"Return chunks for the provided document.\"\"\"\n",
        "        # Simple implementation: one chunk per page\n",
        "        for i, page in enumerate(dl_doc.pages):\n",
        "            text = \" \".join([item.text for item in page.items if hasattr(item, \"text\")])\n",
        "            metadata = {\n",
        "                \"page\": i,\n",
        "                \"source\": dl_doc.name\n",
        "            }\n",
        "            yield BaseChunk(text=text, metadata=metadata)\n",
        "\n",
        "    def serialize(self, chunk: BaseChunk) -> str:\n",
        "        \"\"\"Serialize a chunk for embedding.\"\"\"\n",
        "        # Simple serialization: just return the text\n",
        "        return chunk.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqGb2G5gjAxG"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZyabMjFysjd"
      },
      "source": [
        "## Chunking Strategies Deep Dive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmv0yPhEJSyC"
      },
      "source": [
        "### Strategy 1: HierarchicalChunker - Respecting Document Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrDVxflTPG34"
      },
      "outputs": [],
      "source": [
        "# Import the tokenizer for HybridChunker\n",
        "from transformers import AutoTokenizer\n",
        "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
        "from docling_core.transforms.chunker.hierarchical_chunker import HierarchicalChunker\n",
        "\n",
        "# Set up the tokenizer - using IBM Granite for this example\n",
        "embeddings_model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
        "embeddings_tokenizer = HuggingFaceTokenizer(\n",
        "    tokenizer=AutoTokenizer.from_pretrained(embeddings_model_path),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "iXvpL4g3JSSq",
        "outputId": "683fe8a3-c887-46e2-e122-122570e6bb62"
      },
      "outputs": [],
      "source": [
        "# Create a HierarchicalChunker\n",
        "hierarchical_chunker = HierarchicalChunker()\n",
        "\n",
        "# Generate chunks\n",
        "hierarchical_chunks = list(hierarchical_chunker.chunk(doc))\n",
        "\n",
        "# Visualize the chunks\n",
        "print(f\"Generated {len(hierarchical_chunks)} chunks with HierarchicalChunker\")\n",
        "visualize_chunks(\n",
        "    chunks=hierarchical_chunks,\n",
        "    title=\"HierarchicalChunker Chunks\",\n",
        "    chunker=hierarchical_chunker,\n",
        "    tokenizer=embeddings_tokenizer,\n",
        ")\n",
        "\n",
        "# Examine chunk structure\n",
        "sample_chunk = hierarchical_chunks[2]\n",
        "print(f\"\\nSample Chunk Analysis:\")\n",
        "print(f\"Text (first 200 chars): {sample_chunk.text[:200]}...\")\n",
        "print(f\"Chunk type: {type(sample_chunk).__name__}\")\n",
        "\n",
        "# Print available metadata\n",
        "if hasattr(sample_chunk, 'dl_meta'):\n",
        "    print(\"Document metadata available in 'dl_meta'\")\n",
        "elif hasattr(sample_chunk, 'meta'):\n",
        "    print(f\"Document metadata available in 'meta'\")\n",
        "    print(f\"Meta preview: {str(sample_chunk.meta)[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6NhNEGZMmjE"
      },
      "source": [
        "### Strategy 2: HybridChunker - Balancing Structure and Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "HUkuIu58Motk",
        "outputId": "ca8c5db6-e7a5-4c78-faaa-136a09cf7e6c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import HybridChunker\n",
        "from docling_core.transforms.chunker.hybrid_chunker import HybridChunker\n",
        "\n",
        "# Create a HybridChunker with default settings\n",
        "hybrid_chunker = HybridChunker(\n",
        "    tokenizer=embeddings_tokenizer,\n",
        ")\n",
        "\n",
        "# Generate chunks\n",
        "hybrid_chunks = list(hybrid_chunker.chunk(doc))\n",
        "\n",
        "# Analyze the results\n",
        "print(f\"HybridChunker Results:\")\n",
        "print(f\"Generated {len(hybrid_chunks)} chunks\")\n",
        "\n",
        "visualize_chunks(\n",
        "    chunks=hybrid_chunks,\n",
        "    title=\"HybridChunker: Structure + Size Aware\",\n",
        "    chunker=hybrid_chunker,\n",
        "    tokenizer=embeddings_tokenizer,\n",
        ")\n",
        "\n",
        "# Compare with HierarchicalChunker\n",
        "print(f\"\\nStrategy Comparison:\")\n",
        "print(f\"HierarchicalChunker: {len(hierarchical_chunks)} chunks\")\n",
        "print(f\"HybridChunker: {len(hybrid_chunks)} chunks\")\n",
        "print(f\"Reduction: {((len(hierarchical_chunks) - len(hybrid_chunks)) / len(hierarchical_chunks) * 100):.1f}%\")\n",
        "\n",
        "# Examine a sample chunk with context\n",
        "sample_hybrid_chunk = hybrid_chunks[0]\n",
        "print(f\"\\nSample HybridChunker Chunk:\")\n",
        "print(f\"Text (first 200 chars): {sample_hybrid_chunk.text[:200]}...\")\n",
        "\n",
        "if hasattr(sample_hybrid_chunk, 'meta'):\n",
        "    print(f\"Metadata available - includes structural information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpivbK9wRt6Y"
      },
      "source": [
        "## Advanced Configuration and Fine-Tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry0lZ7UikxrX"
      },
      "source": [
        "### Understanding the Impact of Chunk Size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "id": "M7oAyRDMRuvG",
        "outputId": "9e4b8f96-444f-48f1-ede0-5887f881c5a5"
      },
      "outputs": [],
      "source": [
        "# Create a more constrained tokenizer for demonstration\n",
        "\n",
        "max_tokens=128  # Smaller chunks for fine-grained retrieval\n",
        "\n",
        "adv_tokenizer = HuggingFaceTokenizer(\n",
        "    tokenizer=AutoTokenizer.from_pretrained(embeddings_model_path),\n",
        "    max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "adv_chunker = HybridChunker(\n",
        "    tokenizer=adv_tokenizer,\n",
        ")\n",
        "\n",
        "adv_chunks = list(adv_chunker.chunk(doc))\n",
        "\n",
        "print(f\"Advanced HybridChunker Results (64 token limit):\")\n",
        "print(f\"Generated {len(adv_chunks)} chunks\")\n",
        "\n",
        "visualize_chunks(\n",
        "    chunks=adv_chunks,\n",
        "    title=f\"HybridChunker with {max_tokens} Token Limit\",\n",
        "    chunker=adv_chunker,\n",
        "    tokenizer=adv_tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcUsjKUunl6Q"
      },
      "source": [
        "### The Power of Contextualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDjWAiSHns_A",
        "outputId": "98e42fe5-3c37-4adf-cbd1-84c38c83e837"
      },
      "outputs": [],
      "source": [
        "# Demonstrate contextualization - how chunks get additional context\n",
        "print(f\"\\n Understanding Contextualization:\")\n",
        "print(f\"Contextualization adds relevant surrounding information to improve retrieval quality.\\n\")\n",
        "\n",
        "for i, chunk in enumerate(adv_chunks[:5]):\n",
        "    tokens_text = adv_tokenizer.count_tokens(chunk.text)\n",
        "    contextualized = adv_chunker.contextualize(chunk)\n",
        "    tokens_contextualized = adv_tokenizer.count_tokens(contextualized)\n",
        "\n",
        "    print(f\"Chunk {i}:\")\n",
        "    print(f\"Original text ({tokens_text} tokens): {chunk.text[:100]}...\")\n",
        "    print(f\"Contextualized ({tokens_contextualized} tokens): {contextualized[:100]}...\")\n",
        "    print(f\"Context added: {tokens_contextualized - tokens_text} tokens\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr4AxT0orSwv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcjWtIFZ368O"
      },
      "source": [
        "## Next Steps and Advanced Topics\n",
        "\n",
        "Congratulations! You now understand the fundamentals of document chunking with Docling.\n",
        "\n",
        "### Resources for Further Learning:\n",
        "- [Docling Documentation](https://docling-project.github.io/docling/)\n",
        "- [RAG Best Practices Guide](https://python.langchain.com/docs/tutorials/rag/)\n",
        "- [Chunking Strategies Research](https://arxiv.org/search/?query=text+chunking+retrieval)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
